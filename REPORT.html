<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>REPORT.md - Grip</title>
  <link rel="icon" href="/__/grip/static/favicon.ico" />
  <link rel="stylesheet" href="/__/grip/asset/github-c611305d49c042bc2dcf1dd251e0d1c3.css" />
  <link rel="stylesheet" href="/__/grip/asset/site-aed0c7d68d06ec92057b64d413ec7007.css" />
  <link rel="stylesheet" href="/__/grip/asset/frameworks-146fab5ea30e8afac08dd11013bb4ee0.css" />
  <link rel="stylesheet" href="/__/grip/static/octicons/octicons.css" />
  <style>
    /* Page tweaks */
    .preview-page {
      margin-top: 64px;
    }
    /* User-content tweaks */
    .timeline-comment-wrapper > .timeline-comment:after,
    .timeline-comment-wrapper > .timeline-comment:before {
      content: none;
    }
    /* User-content overrides */
    .discussion-timeline.wide {
      width: 920px;
    }
  </style>
</head>
<body>
  <div class="page">
    <div id="preview-page" class="preview-page" data-autorefresh-url="/__/grip/refresh/">

    

      <div role="main" class="main-content">
        <div class="container new-discussion-timeline experiment-repo-nav">
          <div class="repository-content">
            <div id="readme" class="readme boxed-group clearfix announce instapaper_body md">
              
                <h3>
                  <span class="octicon octicon-book"></span>
                  REPORT.md
                </h3>
              
              <article class="markdown-body entry-content" itemprop="text" id="grip-content">
                <h1>
<a id="user-content-deep-reinforcement-learning-project-navigation" class="anchor" href="#deep-reinforcement-learning-project-navigation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Deep Reinforcement Learning Project: Navigation</h1>
<p>This project aims to train an agent to walk in a virtual environment (Banana Unity environment) and collect as many yellow bananas as possible while avoiding purple ones.</p>
<p>The trained agent in this project is a Deep Q-Networks (DQN) based agent.</p>
<p><a href="https://user-images.githubusercontent.com/10624937/42135619-d90f2f28-7d12-11e8-8823-82b970a54d7e.gif" target="_blank" rel="nofollow"><img src="https://user-images.githubusercontent.com/10624937/42135619-d90f2f28-7d12-11e8-8823-82b970a54d7e.gif" alt="Trained Agent" title="Trained Agent" style="max-width:100%;"></a></p>
<p>This report consists of the following sections:</p>
<ul>
<li>
<p><a href="#1-environment">1. Environment</a></p>
</li>
<li>
<p><a href="#2-implementation">2. Implementation</a></p>
<ul>
<li><a href="#21-deep-q-networks-model">2.1 Deep Q-Networks model</a></li>
<li><a href="#22-experience-replay">2.2 Experience replay</a></li>
<li><a href="#23-dqn-agent">2.3 DQN Agent</a></li>
<li><a href="#24-training">2.4 Training</a></li>
</ul>
</li>
<li>
<p><a href="#3-results">3. Results</a></p>
</li>
<li>
<p><a href="#4-future-works">4. Future work</a></p>
</li>
</ul>
<h2>
<a id="user-content-1-environment" class="anchor" href="#1-environment" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1. Environment</h2>
<p>The environment chosen for the project is similar but not identical to the version of the <a href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#banana-collector">Banana Collector Environment</a> from the Unity ML-Agents toolkit.</p>
<ul>
<li>
<p>A reward of +1 is provided for collecting a yellow banana and a reward of -1 is provided for collecting a blue banana. Thus, the goal of your agent is to collect as many yellow bananas as possible while avoiding blue bananas. Note that whenever the agent collects a banana, a new banana is spawn at a random place in the planar environment.</p>
</li>
<li>
<p>The state space has 37 dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction. Given this information, the agent has to learn how to best select actions. Four discrete actions are available, corresponding to:</p>
<ul>
<li>
<strong><code>0</code></strong> - move forward.</li>
<li>
<strong><code>1</code></strong> - move backward.</li>
<li>
<strong><code>2</code></strong> - turn left.</li>
<li>
<strong><code>3</code></strong> - turn right.</li>
</ul>
</li>
<li>
<p>The task is episodic and the criteria solving the environment is to get <strong>an average score of +13 over 100 consecutive episodes</strong>.</p>
</li>
</ul>
<p>Readers are recommended to read the Deep Q-learning algorithm [1] paper before diving into the project, it presents the theoretical foundations of the models, update rules, tweaks such as experience replay, target networks, soft updates, etc.</p>
<h2>
<a id="user-content-2-implementation" class="anchor" href="#2-implementation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2. Implementation</h2>
<h3>
<a id="user-content-21-deep-q-networks-model" class="anchor" href="#21-deep-q-networks-model" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2.1 Deep Q-Networks model</h3>
<p>DQN is implemented as a neural network consisting of 1 input layer, 2 fully-connected hidden layers, and 1 output layer. The network can be described as follow:</p>
<ul>
<li>Input layer: 37 neurons (the state size)</li>
<li>Fully-connected layer 1: 64 neurons, activation = ReLU</li>
<li>Fully-connected layer 2: 64 neurons, activation = ReLU</li>
<li>Output layer: 4 neurons (the action size)</li>
</ul>
<p>This follows the <a href="https://github.com/udacity/deep-reinforcement-learning/tree/master/dqn/solution">Udacity Q-network implementation</a>. The implementation can be found under the <code>dqn_model.py</code> file.</p>
<div class="highlight highlight-source-python"><pre><span class="pl-k">class</span> <span class="pl-en">DQN</span>(<span class="pl-e">nn</span>.<span class="pl-e">Module</span>):
    <span class="pl-s"><span class="pl-pds">"""</span>Actor (Policy) Model<span class="pl-pds">"""</span></span>

    <span class="pl-k">def</span> <span class="pl-c1">__init__</span>(<span class="pl-smi"><span class="pl-smi">self</span></span>, <span class="pl-smi">state_size</span>, <span class="pl-smi">action_size</span>, <span class="pl-smi">seed</span>, <span class="pl-smi">fc1_units</span><span class="pl-k">=</span><span class="pl-c1">64</span>, <span class="pl-smi">fc2_units</span><span class="pl-k">=</span><span class="pl-c1">64</span>):
        <span class="pl-s"><span class="pl-pds">"""</span>Initialize parameters and build model.</span>
<span class="pl-s">        Params</span>
<span class="pl-s">        ======</span>
<span class="pl-s">            state_size (int): Dimension of each state</span>
<span class="pl-s">            action_size (int): Dimension of each action</span>
<span class="pl-s">            seed (int): Random seed</span>
<span class="pl-s">            fc1_units (int): Number of nodes in first hidden layer</span>
<span class="pl-s">            fc2_units (int): Number of nodes in second hidden layer</span>
<span class="pl-s">        <span class="pl-pds">"""</span></span>
        <span class="pl-c1">super</span>(<span class="pl-c1">DQN</span>, <span class="pl-c1">self</span>).<span class="pl-c1">__init__</span>()
        <span class="pl-c1">self</span>.seed <span class="pl-k">=</span> torch.manual_seed(seed)
        <span class="pl-c1">self</span>.fc1 <span class="pl-k">=</span> nn.Linear(state_size, fc1_units)
        <span class="pl-c1">self</span>.fc2 <span class="pl-k">=</span> nn.Linear(fc1_units, fc2_units)
        <span class="pl-c1">self</span>.fc3 <span class="pl-k">=</span> nn.Linear(fc2_units, action_size)

    <span class="pl-k">def</span> <span class="pl-en">forward</span>(<span class="pl-smi"><span class="pl-smi">self</span></span>, <span class="pl-smi">state</span>):
        <span class="pl-s"><span class="pl-pds">"""</span>Build a network that maps state -&gt; action values.<span class="pl-pds">"""</span></span>
        x <span class="pl-k">=</span> F.relu(<span class="pl-c1">self</span>.fc1(state))
        x <span class="pl-k">=</span> F.relu(<span class="pl-c1">self</span>.fc2(x))
        <span class="pl-k">return</span> <span class="pl-c1">self</span>.fc3(x)</pre></div>
<h3>
<a id="user-content-22-experience-replay" class="anchor" href="#22-experience-replay" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2.2 Experience replay</h3>
<p>When the agent interacts with the environment, the sequence of experience tuples can be highly correlated. The naive Q-learning algorithm that learns from each of these experience tuples in sequential order runs the risk of getting swayed by the effects of this correlation. By instead keeping track of a  <strong>replay buffer</strong>  and using  <strong>experience replay</strong>  to sample from the buffer at random, we can prevent action values from oscillating or diverging catastrophically.</p>
<p>The  <strong>replay buffer</strong>  contains a collection of experience tuples (SS,  AA,  RR,  S′S′). The tuples are gradually added to the buffer as we are interacting with the environment.</p>
<p>The act of sampling a small batch of tuples from the replay buffer in order to learn is known as  <strong>experience replay</strong>. In addition to breaking harmful correlations, experience replay allows us to learn more from individual tuples multiple times, recall rare occurrences, and in general make better use of our experience.</p>
<p>The implementation of the <strong>Replay Buffer</strong> can be found under the <code>dqn_agent.py</code> file as <code>RelayBuffer</code> class.</p>
<div class="highlight highlight-source-python"><pre><span class="pl-k">class</span> <span class="pl-en">ReplayBuffer</span>:
    <span class="pl-s"><span class="pl-pds">"""</span>Fixed-size buffer to store experience tuples.<span class="pl-pds">"""</span></span>

    <span class="pl-k">def</span> <span class="pl-c1">__init__</span>(<span class="pl-smi"><span class="pl-smi">self</span></span>, <span class="pl-smi">action_size</span>, <span class="pl-smi">replay_buffer_size</span>, <span class="pl-smi">minibatch_size</span>, <span class="pl-smi">seed</span>):
        <span class="pl-s"><span class="pl-pds">"""</span>Initialize a ReplayBuffer object.</span>
<span class="pl-s"></span>
<span class="pl-s">        Params</span>
<span class="pl-s">        ======</span>
<span class="pl-s">            action_size (int): dimension of each action</span>
<span class="pl-s">            buffer_size (int): maximum size of buffer</span>
<span class="pl-s">            batch_size (int): size of each training batch</span>
<span class="pl-s">            seed (int): random seed</span>
<span class="pl-s">        <span class="pl-pds">"""</span></span>
        <span class="pl-c1">self</span>.action_size <span class="pl-k">=</span> action_size
        <span class="pl-c1">self</span>.memory <span class="pl-k">=</span> deque(<span class="pl-v">maxlen</span><span class="pl-k">=</span>replay_buffer_size)
        <span class="pl-c1">self</span>.minibatch_size <span class="pl-k">=</span> minibatch_size
        <span class="pl-c1">self</span>.experience <span class="pl-k">=</span> namedtuple(<span class="pl-s"><span class="pl-pds">"</span>Experience<span class="pl-pds">"</span></span>, <span class="pl-v">field_names</span><span class="pl-k">=</span>[<span class="pl-s"><span class="pl-pds">"</span>state<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>action<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>reward<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>next_state<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>done<span class="pl-pds">"</span></span>])
        <span class="pl-c1">self</span>.seed <span class="pl-k">=</span> random.seed(seed)

    <span class="pl-k">def</span> <span class="pl-en">add</span>(<span class="pl-smi"><span class="pl-smi">self</span></span>, <span class="pl-smi">state</span>, <span class="pl-smi">action</span>, <span class="pl-smi">reward</span>, <span class="pl-smi">next_state</span>, <span class="pl-smi">done</span>):
        <span class="pl-s"><span class="pl-pds">"""</span>Add a new experience to memory<span class="pl-pds">"""</span></span>
        e <span class="pl-k">=</span> <span class="pl-c1">self</span>.experience(state, action, reward, next_state, done)
        <span class="pl-c1">self</span>.memory.append(e)

    <span class="pl-k">def</span> <span class="pl-en">sample</span>(<span class="pl-smi"><span class="pl-smi">self</span></span>):
        <span class="pl-s"><span class="pl-pds">"""</span>Randomly sample a batch of experiences from memory<span class="pl-pds">"""</span></span>
        experiences <span class="pl-k">=</span> random.sample(<span class="pl-c1">self</span>.memory, <span class="pl-v">k</span><span class="pl-k">=</span><span class="pl-c1">self</span>.minibatch_size)

        states <span class="pl-k">=</span> torch.from_numpy(np.vstack([e.state <span class="pl-k">for</span> e <span class="pl-k">in</span> experiences <span class="pl-k">if</span> e <span class="pl-k">is</span> <span class="pl-k">not</span> <span class="pl-c1">None</span>])).float().to(device)
        actions <span class="pl-k">=</span> torch.from_numpy(np.vstack([e.action <span class="pl-k">for</span> e <span class="pl-k">in</span> experiences <span class="pl-k">if</span> e <span class="pl-k">is</span> <span class="pl-k">not</span> <span class="pl-c1">None</span>])).long().to(device)
        rewards <span class="pl-k">=</span> torch.from_numpy(np.vstack([e.reward <span class="pl-k">for</span> e <span class="pl-k">in</span> experiences <span class="pl-k">if</span> e <span class="pl-k">is</span> <span class="pl-k">not</span> <span class="pl-c1">None</span>])).float().to(device)
        next_states <span class="pl-k">=</span> torch.from_numpy(np.vstack([e.next_state <span class="pl-k">for</span> e <span class="pl-k">in</span> experiences <span class="pl-k">if</span> e <span class="pl-k">is</span> <span class="pl-k">not</span> <span class="pl-c1">None</span>])).float().to(
            device)
        dones <span class="pl-k">=</span> torch.from_numpy(np.vstack([e.done <span class="pl-k">for</span> e <span class="pl-k">in</span> experiences <span class="pl-k">if</span> e <span class="pl-k">is</span> <span class="pl-k">not</span> <span class="pl-c1">None</span>]).astype(np.uint8)).float().to(
            device)

        <span class="pl-k">return</span> (states, actions, rewards, next_states, dones)

    <span class="pl-k">def</span> <span class="pl-c1">__len__</span>(<span class="pl-smi"><span class="pl-smi">self</span></span>):
        <span class="pl-s"><span class="pl-pds">"""</span>Return the current size of internal memory.<span class="pl-pds">"""</span></span>
        <span class="pl-k">return</span> <span class="pl-c1">len</span>(<span class="pl-c1">self</span>.memory)</pre></div>
<h3>
<a id="user-content-23-dqn-agent" class="anchor" href="#23-dqn-agent" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2.3 DQN Agent</h3>
<p>With the above implementations, the DQN agent is implemented in <code>dqn_agent.py</code> as <code>DQNAgent</code> class.</p>
<ul>
<li>Replay Bufer is initialized to store the memory of the agent (<code>DQNAgent.memory</code>)</li>
<li>Local network (<code>DQNAgent.dqn_local</code>) and Target network (<code>DQNAgent.dqn_target</code>) are initialized based on the model defined above
<ul>
<li>
<code>DQNAgent.dqn_local:</code> this network is used for the training process</li>
<li>
<code>DQNAgent.dqn_target:</code> this network is used to store the weights of the local network, and produce the action values to makes the training stable</li>
</ul>
</li>
<li>
<code>DQNAgent.act()</code> returns actions for given state as per current policy. This is implemented using an Epsilon-greedy selection so that to balance between <em>exploration</em> and <em>exploitation</em> for the Q Learning)</li>
<li>
<code>DQNAgent.step()</code>
<ul>
<li>Store a step taken by the agent (state, action, reward, next_state, done) in the Memory</li>
<li>Every 4 steps (and if their are enough experiences available in the Replay Buffer), sample the experiences from the Replay Buffer and <code>learn()</code> from that</li>
</ul>
</li>
<li>
<code>DQNAgent.learn()</code> which minimize the loss of the expected Q-value and target Q-value. The Q-target and<code>soft_update()</code> the network</li>
<li>
<code>DQNAgent.soft_update()</code> copy the weights of the local network to the target network based on the rule described below:
<a href="https://user-images.githubusercontent.com/18066876/79749721-fcf6c400-830f-11ea-9bb1-5295231b73c5.png" target="_blank" rel="nofollow"><img src="https://user-images.githubusercontent.com/18066876/79749721-fcf6c400-830f-11ea-9bb1-5295231b73c5.png" alt="image" style="max-width:100%;"></a>
</li>
</ul>
<h4>
<a id="user-content-hyper-parameters" class="anchor" href="#hyper-parameters" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Hyper Parameters</h4>
<p>The hyperparameters are chosen as follows:</p>
<div class="highlight highlight-source-python"><pre><span class="pl-c1">REPLAY_BUFFER_SIZE</span> <span class="pl-k">=</span> <span class="pl-c1">int</span>(<span class="pl-c1">1e5</span>)   <span class="pl-c"><span class="pl-c">#</span> size of our agent's memory</span>
<span class="pl-c1">MINIBATCH_SIZE</span> <span class="pl-k">=</span> <span class="pl-c1">64</span>             <span class="pl-c"><span class="pl-c">#</span> size of the training batch</span>
<span class="pl-c1">GAMMA</span> <span class="pl-k">=</span> <span class="pl-c1">0.99</span>                    <span class="pl-c"><span class="pl-c">#</span> discount factor</span>
<span class="pl-c1">TAU</span> <span class="pl-k">=</span> <span class="pl-c1">1e-3</span>                      <span class="pl-c"><span class="pl-c">#</span> for soft update of target parameters</span>
<span class="pl-c1">LR</span> <span class="pl-k">=</span> <span class="pl-c1">5e-4</span>                       <span class="pl-c"><span class="pl-c">#</span> learning rate</span>
<span class="pl-c1">UPDATE_EVERY</span> <span class="pl-k">=</span> <span class="pl-c1">4</span>                <span class="pl-c"><span class="pl-c">#</span> soft update the network after every number of timesteps</span></pre></div>
<p>The hyperparameters are self-described.</p>
<h3>
<a id="user-content-24-training" class="anchor" href="#24-training" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2.4 Training</h3>
<p>The training process is implemented in the Navigation.ipynb notebook. The algorithm follows the process described in the <a href="https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893" rel="nofollow">Deep Reinforement Learning Nanodegree</a>.</p>
<p><a href="./DQN.png" target="_blank" rel="noopener noreferrer"><img src="./DQN.png" alt="DQN-Algorithm" style="max-width:100%;"></a></p>
<div class="highlight highlight-source-python"><pre><span class="pl-k">def</span> <span class="pl-en">train_dqn</span>(<span class="pl-smi">n_episodes</span><span class="pl-k">=</span><span class="pl-c1">2000</span>, <span class="pl-smi">max_t</span><span class="pl-k">=</span><span class="pl-c1">1000</span>, <span class="pl-smi">eps_start</span><span class="pl-k">=</span><span class="pl-c1">1.0</span>, <span class="pl-smi">eps_end</span><span class="pl-k">=</span><span class="pl-c1">0.01</span>, <span class="pl-smi">eps_decay</span><span class="pl-k">=</span><span class="pl-c1">0.995</span>):
    <span class="pl-s"><span class="pl-pds">"""</span>Deep Q-Learning.</span>
<span class="pl-s">    </span>
<span class="pl-s">    Params</span>
<span class="pl-s">    ======</span>
<span class="pl-s">        n_episodes (int): maximum number of training episodes</span>
<span class="pl-s">        max_t (int): maximum number of timesteps per episode</span>
<span class="pl-s">        eps_start (float): starting value of epsilon, for epsilon-greedy action selection</span>
<span class="pl-s">        eps_end (float): minimum value of epsilon</span>
<span class="pl-s">        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon</span>
<span class="pl-s">    <span class="pl-pds">"""</span></span>
    scores <span class="pl-k">=</span> []                             <span class="pl-c"><span class="pl-c">#</span> keep track of all the scores</span>
    scores_window <span class="pl-k">=</span> deque(<span class="pl-v">maxlen</span><span class="pl-k">=</span><span class="pl-c1">100</span>)       <span class="pl-c"><span class="pl-c">#</span> keep track of the last 100 scores</span>
    eps <span class="pl-k">=</span> eps_start
    <span class="pl-k">for</span> i_episode <span class="pl-k">in</span> <span class="pl-c1">range</span>(<span class="pl-c1">1</span>, n_episodes <span class="pl-k">+</span> <span class="pl-c1">1</span>):
        env_info <span class="pl-k">=</span> env.reset(<span class="pl-v">train_mode</span><span class="pl-k">=</span><span class="pl-c1">True</span>)[brain_name]
        state <span class="pl-k">=</span> env_info.vector_observations[<span class="pl-c1">0</span>]
        score <span class="pl-k">=</span> <span class="pl-c1">0</span>
        <span class="pl-k">for</span> t <span class="pl-k">in</span> <span class="pl-c1">range</span>(max_t):
            action <span class="pl-k">=</span> agent.act(state, eps)
            env_info <span class="pl-k">=</span> env.step(action)[brain_name]
            
            next_state <span class="pl-k">=</span> env_info.vector_observations[<span class="pl-c1">0</span>]
            reward <span class="pl-k">=</span> env_info.rewards[<span class="pl-c1">0</span>]
            done <span class="pl-k">=</span> env_info.local_done[<span class="pl-c1">0</span>]
            
            agent.step(state, action, reward, next_state, done)
            state <span class="pl-k">=</span> next_state
            score <span class="pl-k">+=</span> reward
            <span class="pl-k">if</span> done:
                <span class="pl-k">break</span>
        scores_window.append(score)          <span class="pl-c"><span class="pl-c">#</span> save most recent score</span>
        scores.append(score)                 <span class="pl-c"><span class="pl-c">#</span> save most recent score</span>
        eps <span class="pl-k">=</span> <span class="pl-c1">max</span>(eps_end, eps <span class="pl-k">*</span> eps_decay)  <span class="pl-c"><span class="pl-c">#</span> decrease epsilon</span>
    
        <span class="pl-c1">print</span>(<span class="pl-s"><span class="pl-pds">'</span><span class="pl-cce">\r</span>Episode <span class="pl-c1">{}</span><span class="pl-cce">\t</span>Average Score: <span class="pl-c1">{<span class="pl-k">:.2f</span>}</span><span class="pl-pds">'</span></span>.format(i_episode, np.mean(scores_window)), <span class="pl-v">end</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>)
        <span class="pl-k">if</span> i_episode <span class="pl-k">%</span> <span class="pl-c1">100</span> <span class="pl-k">==</span> <span class="pl-c1">0</span>:
            <span class="pl-c1">print</span>(<span class="pl-s"><span class="pl-pds">'</span><span class="pl-cce">\r</span>Episode <span class="pl-c1">{}</span><span class="pl-cce">\t</span>Average Score: <span class="pl-c1">{<span class="pl-k">:.2f</span>}</span><span class="pl-pds">'</span></span>.format(i_episode, np.mean(scores_window)))
        <span class="pl-k">if</span> np.mean(scores_window) <span class="pl-k">&gt;=</span> <span class="pl-c1">BENCHMARK_SCORE</span>:
            <span class="pl-c1">print</span>(<span class="pl-s"><span class="pl-pds">'</span><span class="pl-cce">\n</span>Environment solved in <span class="pl-c1">{<span class="pl-k">:d</span>}</span> episodes!<span class="pl-cce">\t</span>Average Score: <span class="pl-c1">{<span class="pl-k">:.2f</span>}</span><span class="pl-pds">'</span></span>.format(i_episode <span class="pl-k">-</span> <span class="pl-c1">100</span>,
                                                                                         np.mean(scores_window)))
            torch.save(agent.dqn_local.state_dict(), <span class="pl-s"><span class="pl-pds">'</span>checkpoint.pth<span class="pl-pds">'</span></span>)
            <span class="pl-k">break</span>

    <span class="pl-k">return</span> scores</pre></div>
<h2>
<a id="user-content-3-results" class="anchor" href="#3-results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>3. Results</h2>
<p>This section presents the result of training and testing our DQN agent.</p>
<h4>
<a id="user-content-training-scores" class="anchor" href="#training-scores" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Training scores</h4>
<p><a href="./result.png" target="_blank" rel="noopener noreferrer"><img src="./result.png" alt="training_result" style="max-width:100%;"></a></p>
<h2>
<a id="user-content-4-future-works" class="anchor" href="#4-future-works" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>4. Future works</h2>
<p>As discussed in the Deep Reinforement Learning course, we can improve our Deep Q-Networks by implementing some additional techniques:</p>
<ul>
<li><a href="https://arxiv.org/abs/1509.06461" rel="nofollow">Double DQN</a></li>
<li><a href="https://arxiv.org/abs/1511.06581" rel="nofollow">Dueling DQN</a></li>
<li><a href="https://arxiv.org/abs/1511.05952" rel="nofollow">Prioritized experience delay</a></li>
</ul>

              </article>
            </div>
          </div>
        </div>
      </div>

    

  </div>
  <div>&nbsp;</div>
  </div><script>
    function showCanonicalImages() {
      var images = document.getElementsByTagName('img');
      if (!images) {
        return;
      }
      for (var index = 0; index < images.length; index++) {
        var image = images[index];
        if (image.getAttribute('data-canonical-src') && image.src !== image.getAttribute('data-canonical-src')) {
          image.src = image.getAttribute('data-canonical-src');
        }
      }
    }

    function scrollToHash() {
      if (location.hash && !document.querySelector(':target')) {
        var element = document.getElementById('user-content-' + location.hash.slice(1));
        if (element) {
           element.scrollIntoView();
        }
      }
    }

    function autorefreshContent(eventSourceUrl) {
      var initialTitle = document.title;
      var contentElement = document.getElementById('grip-content');
      var source = new EventSource(eventSourceUrl);
      var isRendering = false;

      source.onmessage = function(ev) {
        var msg = JSON.parse(ev.data);
        if (msg.updating) {
          isRendering = true;
          document.title = '(Rendering) ' + document.title;
        } else {
          isRendering = false;
          document.title = initialTitle;
          contentElement.innerHTML = msg.content;
          showCanonicalImages();
        }
      }

      source.onerror = function(e) {
        if (e.readyState === EventSource.CLOSED && isRendering) {
          isRendering = false;
          document.title = initialTitle;
        }
      }
    }

    window.onhashchange = function() {
      scrollToHash();
    }

    window.onload = function() {
      scrollToHash();
    }

    showCanonicalImages();

    var autorefreshUrl = document.getElementById('preview-page').getAttribute('data-autorefresh-url');
    if (autorefreshUrl) {
      autorefreshContent(autorefreshUrl);
    }
  </script>
</body>
</html>